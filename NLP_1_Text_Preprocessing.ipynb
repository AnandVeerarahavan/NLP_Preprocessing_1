{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_1_Text Preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfe3qBss0HoR"
      },
      "source": [
        "NLTK - Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkGIYwbmz2te",
        "outputId": "10a79593-5e1d-4e2c-c144-89f7dc9641b9"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsIatWui0F0x"
      },
      "source": [
        "Importing and download NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ty_45rc0Wdk",
        "outputId": "c388a7fc-4c5d-4a38-95a5-0f7ce13e0d24"
      },
      "source": [
        "import nltk\n",
        "nltk.download() "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> q\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> l\n",
            "\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: q\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgeY1oUc0mKT"
      },
      "source": [
        "Download gutenberg. Gutenberg is a collection of literary works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJObv5oL0ckh",
        "outputId": "2a0002a0-3de3-460d-b8e8-fe1928945c2c"
      },
      "source": [
        "nltk.download('gutenberg')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jlQ_aVR010O"
      },
      "source": [
        "Download genesis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV7oZYlU0q05",
        "outputId": "ad78f729-c373-43a1-84f6-03132bf7f9ae"
      },
      "source": [
        "nltk.download('genesis')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/genesis.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcvKpbRQ1LXQ"
      },
      "source": [
        "To see what content is available in gutenberg."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLUo0V-X1ADA",
        "outputId": "85327afd-341a-48bb-b5f8-1eec7f19a4c4"
      },
      "source": [
        "nltk.corpus.gutenberg.fileids()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5J346pM1Wm-"
      },
      "source": [
        "Resources for datasets : 1. Kaggle 2. Huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqyopd4-1fWH"
      },
      "source": [
        "To get an example of what text it contains- gutenberg - whitman_leaves.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oav6gHJ_1TxG",
        "outputId": "e1a2a301-aaf3-4511-cf66-1089b3c33e7c"
      },
      "source": [
        "whitman_content = nltk.corpus.gutenberg.words('whitman-leaves.txt')\n",
        "print(whitman_content)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[', 'Leaves', 'of', 'Grass', 'by', 'Walt', 'Whitman', ...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgLMRleb1zKc"
      },
      "source": [
        "Other packages that are avaliable are :\n",
        "1. inaugral\n",
        "2. nps_chat\n",
        "3. webtext\n",
        "4. treebank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPOQbsyR1wGN",
        "outputId": "2a18697f-651f-4254-c996-180162911c72"
      },
      "source": [
        "nltk.download('inaugural')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/inaugural.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OZ8UzQ_1_JF",
        "outputId": "c9047025-fcd3-4b9b-e161-ac0d01401e4c"
      },
      "source": [
        "nltk.download('nps_chat')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nps_chat.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br5PGx-X2KKl",
        "outputId": "51ce10b6-c7b5-428e-efbb-1cea42c0f031"
      },
      "source": [
        "nltk.download('webtext')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2KHOdg72NN0",
        "outputId": "5d84b8ce-166e-4904-c808-3ad7604ee71b"
      },
      "source": [
        "nltk.download('treebank')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxqmBarL2SSx"
      },
      "source": [
        "Import all the books from tree bank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxlNnPXj2Pvd",
        "outputId": "85efb210-f4eb-48d0-ab31-48c568355549"
      },
      "source": [
        "from nltk.book import *"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKDHOT_F2dhH"
      },
      "source": [
        "To find the length of a book"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In0eviVE2bX0",
        "outputId": "ff465416-ad4a-4337-a159-8996264c09bd"
      },
      "source": [
        "no_of_words_moby_dick = len(text1)\n",
        "print(no_of_words_moby_dick)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "260819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YknjeFtU3EN5"
      },
      "source": [
        "Basic steps of pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiBAS_6m3KWc"
      },
      "source": [
        "Text Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p85pS3N43Odm"
      },
      "source": [
        "Removing punctuations, accent marks and special symbols and diacritics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "t7ELXR-D2mlz",
        "outputId": "f04bc894-43b8-4c50-b76d-0c7d17472b0a"
      },
      "source": [
        "import re\n",
        "def remove_regex(input_text, regex_pattern):\n",
        "  \n",
        "  urls = re.finditer(regex_pattern, input_text)\n",
        "  \n",
        "  for i in urls:\n",
        "    input_text = re.sub(i.group().strip(),'',input_text)\n",
        "  return input_text\n",
        "  \n",
        "regex_pattern =\"#[\\w]*\"\n",
        "  \n",
        "remove_regex(\"Hey there! I am removing this #Hashtag from the sentence\", regex_pattern)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hey there! I am removing this  from the sentence'"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxKo5itU4UAw"
      },
      "source": [
        "Removing whitespaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRUPaE-P21La",
        "outputId": "48087eaa-560c-4d7b-9d6f-205be6082a76"
      },
      "source": [
        "input_str = \"\\t A string example\\t\"\n",
        "print(\"Before removing whitespace -----\", input_str)\n",
        "\n",
        "input_str = input_str.strip()\n",
        "print(\"After removing whitespace -----\", input_str) "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before removing whitespace ----- \t A string example\t\n",
            "After removing whitespace ----- A string example\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-Nm5hYe40OS"
      },
      "source": [
        "Removing space between words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd2TrD3Q4q_g",
        "outputId": "ea684925-dcf2-4642-8a64-aca8dfc2635a"
      },
      "source": [
        "print(input_str.split())"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'string', 'example']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_hqWn6U4_Mg"
      },
      "source": [
        "Removing or replacing numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ef59Up-247c2",
        "outputId": "8ee2ff08-ee51-44d4-d0d3-6aefc2cf3860"
      },
      "source": [
        "regex_pattern = \"[0-9]+\"\n",
        "\n",
        "input_txt = '1 banana 22 apples'\n",
        "\n",
        "remove_regex(input_txt, regex_pattern)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' banana  apples'"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DghL_rtQ7tiQ"
      },
      "source": [
        "Convert numbers to words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOm7UYt77xq0"
      },
      "source": [
        "For this we use a library called num2words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CMGW9ly72rW",
        "outputId": "650bf8ea-4e82-41ba-da57-b74f319f7107"
      },
      "source": [
        "pip install num2words"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting num2words\n",
            "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▎                            | 10 kB 22.4 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 30 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 51 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 61 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 71 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 81 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 92 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 101 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words) (0.6.2)\n",
            "Installing collected packages: num2words\n",
            "Successfully installed num2words-0.5.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xImqZ_2u7xoa"
      },
      "source": [
        "Convert numbers to words using num2words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFiUwU605RLV",
        "outputId": "5881eb39-b6e4-44ae-88d3-21ba32d8d395"
      },
      "source": [
        "from num2words import num2words\n",
        "print(num2words(456))\n",
        "print(num2words(77))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "four hundred and fifty-six\n",
            "seventy-seven\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0qoITWf8I5u",
        "outputId": "022cdbb1-786a-46f0-810e-705141fc4732"
      },
      "source": [
        "import re\n",
        "input_str = ' Box A contains 3 red , 5 blue and 17 green balls'\n",
        "out = ' '.join((num2words(i) if i.isdigit() else i for i in input_str.split()))\n",
        "print(out)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box A contains three red , five blue and seventeen green balls\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpKAoluz8nLX"
      },
      "source": [
        "Convert uppercase to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbzEAEEl8iC1",
        "outputId": "9180ac18-d6a3-4c29-84ea-d4102109782d"
      },
      "source": [
        "input_str = \"There were 5 countries that participated in the meet, they are Australia, New Zealand, India, US and China\"\n",
        "\n",
        "input_str = input_str.lower()\n",
        "\n",
        "print(input_str)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there were 5 countries that participated in the meet, they are australia, new zealand, india, us and china\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZmDkZFZ9HDi"
      },
      "source": [
        "Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJH4kK7X9KLR"
      },
      "source": [
        "Install punkt package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_x8jFu09CLk",
        "outputId": "7967bc70-ffce-4b4d-8ed8-509df4f487f9"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC8-_nB19Tsi"
      },
      "source": [
        "Process of splitting given text into smaller pieces called tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SoE9Qfy9P3z",
        "outputId": "0f4a4935-47f7-4375-9489-dabc2910be46"
      },
      "source": [
        "input_str = 'NLTK is a leading platform for python'\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(input_str)\n",
        "\n",
        "print(\" Tokens ---\",tokens)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Tokens --- ['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuc1Kyh69wR0"
      },
      "source": [
        "Remove stop words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx8Fq2XG928C"
      },
      "source": [
        "Download library stopwords from nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLrf5mrJ9uGh",
        "outputId": "c31cf343-d46b-421c-c86a-62a474d493a2"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSm9mUad91pQ",
        "outputId": "69cf66f0-0fcc-4fed-ecde-1c88bbb0d205"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "print('Stop words are -----> ', stop_words)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop words are ----->  {'between', 'had', 'while', 'have', 'until', 'no', 'i', 'she', 'an', 'by', 'when', 'having', 'what', 'these', 'couldn', 'there', 'here', 'ain', \"shouldn't\", 're', \"mightn't\", 'wasn', 'after', 'above', 'herself', 'because', 'during', 'of', \"hadn't\", 'those', 'into', 'too', 'himself', 'shan', 'itself', 'its', 'to', 'won', 'needn', 'same', 's', 'aren', 'shouldn', 'hers', 'which', 'her', 'yourself', 'own', 'weren', 'over', 'them', \"weren't\", 'under', 'before', 'or', 'they', 'more', \"haven't\", \"isn't\", \"aren't\", 'hasn', 'for', \"shan't\", 'through', 'were', \"don't\", 'our', 'did', \"hasn't\", \"you've\", \"doesn't\", 'mightn', 'me', 'with', 'his', 'being', 'been', 'just', 'all', 've', 'do', 'd', 'most', 'should', \"wasn't\", 'yours', 'has', 'isn', 'you', 'now', 'hadn', \"you'd\", 'ours', 'doing', 'below', 'off', 'once', 'be', 'but', 'down', 'again', \"you'll\", 'then', \"won't\", \"it's\", \"needn't\", 'can', 'at', 'on', 'up', 'my', 'very', 'haven', 'are', 'ma', 'ourselves', 'not', 'wouldn', 'themselves', 'so', 'their', 'he', 'that', 'it', \"couldn't\", 'didn', 'nor', 'some', 't', 'how', 'will', 'y', 'myself', 'against', 'each', 'your', 'does', 'both', 'theirs', 'doesn', 'a', 'from', 'll', 'am', 'only', \"she's\", 'further', \"mustn't\", \"wouldn't\", 'yourselves', 'whom', 'we', 'and', 'as', 'don', 'in', 'if', 'any', 'where', 'm', 'why', 'was', 'about', 'out', 'than', \"should've\", 'this', 'is', \"didn't\", 'who', \"you're\", 'other', \"that'll\", 'such', 'mustn', 'o', 'him', 'few', 'the'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQJdPkyh-UXR"
      },
      "source": [
        "Sk learn stop words library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZSC59hu-Oa5",
        "outputId": "29406721-8d12-4e91-9cb0-1b0ad26649a5"
      },
      "source": [
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "\n",
        "print(\"sklearn stop words -------->\",ENGLISH_STOP_WORDS)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sklearn stop words --------> frozenset({'twelve', 'every', 'while', 'have', 'others', 'detail', 'until', 'hereafter', 'no', 'she', 'made', 'ie', 'describe', 'towards', 'whole', 'becoming', 'co', 'ltd', 'of', 'de', 'to', 'together', 'front', 'name', 'twenty', 'her', 'upon', 'found', 'or', 'toward', 'hasnt', 'through', 'whereas', 'five', 'our', 'via', 'bill', 'around', 'should', 'seeming', 'now', 'without', 'off', 'keep', 'move', 'but', 'empty', 'then', 'at', 'are', 'so', 'he', 'it', 'us', 'already', 'nor', 'how', 'none', 'myself', 'beside', 'each', 'both', 'something', 'con', 'two', 'almost', 'a', 'whereupon', 'least', 'full', 'whom', 'might', 'see', 'any', 'whereby', 'besides', 'nine', 'out', 'four', 'is', 'becomes', 'thru', 'who', 'such', 'fill', 'whether', 'thin', 'third', 'show', 'anyhow', 'forty', 'whenever', 'an', 'well', 'what', 'there', 'here', 'rather', 'meanwhile', 'among', 're', 'inc', 'after', 'above', 'therein', 'because', 'those', 'too', 'fire', 'its', 'therefore', 'many', 'sixty', 'anything', 'yourself', 'eleven', 'they', 'more', 'across', 'for', 'interest', 'formerly', 'his', 'being', 'all', 'thereafter', 'enough', 'always', 'ours', 'below', 'down', 'mill', 'on', 'onto', 'perhaps', 'however', 'further', 'yourselves', 'cant', 'though', 'hence', 'where', 'etc', 'get', 'other', 'sincere', 'the', 'give', 'had', 'would', 'serious', 'i', 'call', 'former', 'anyone', 'anyway', 'beforehand', 'herself', 'amongst', 'during', 'much', 'himself', 'fifty', 'itself', 'ten', 'often', 'same', 'another', 'hers', 'which', 'otherwise', 'afterwards', 'namely', 'over', 'under', 'before', 'were', 'me', 'with', 'still', 'behind', 'has', 'indeed', 'everyone', 'you', 'take', 'can', 'within', 'up', 'somewhere', 'not', 'part', 'that', 'some', 'hereby', 'somehow', 'elsewhere', 'against', 'your', 'find', 'amount', 'due', 'from', 'whoever', 'moreover', 'alone', 'seem', 'bottom', 'nobody', 'and', 'done', 'we', 'in', 'latterly', 'go', 'yet', 'someone', 'first', 'about', 'eg', 'this', 'neither', 'next', 'six', 'hereupon', 'either', 'him', 'several', 'few', 'please', 'between', 'latter', 'by', 'when', 'these', 'nevertheless', 'thus', 'ever', 'although', 'into', 'along', 'anywhere', 'thence', 'un', 'sometime', 'seems', 'thereupon', 'may', 'also', 'hundred', 'less', 'even', 'own', 'wherein', 'them', 'whither', 'three', 'cannot', 'system', 'thick', 'throughout', 'whose', 'whence', 'nowhere', 'whereafter', 'everything', 'sometimes', 'never', 'become', 'been', 'fifteen', 'do', 'one', 'most', 'except', 'beyond', 'could', 'yours', 'thereby', 'once', 'back', 'be', 'again', 'top', 'my', 'very', 'ourselves', 'eight', 'themselves', 'nothing', 'last', 'couldnt', 'whatever', 'their', 'put', 'will', 'noone', 'am', 'only', 'amoungst', 'side', 'wherever', 'as', 'if', 'since', 'why', 'was', 'than', 'per', 'mine', 'mostly', 'seemed', 'herein', 'everywhere', 'cry', 'became', 'must', 'else'})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3QyvLeI-qry"
      },
      "source": [
        "STEMMING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2tabFSw-uF4"
      },
      "source": [
        "Porter Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1YAhTPX-k-4",
        "outputId": "e1b0cff9-df2a-494b-b3cd-50012658c997"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "input_str = \"There are several types of stemming algorithms that are available\"\n",
        "input_str = word_tokenize(input_str)\n",
        "print(\"Stem words :\\n\")\n",
        "for word in input_str:\n",
        "  print(stemmer.stem(word)) "
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stem words :\n",
            "\n",
            "there\n",
            "are\n",
            "sever\n",
            "type\n",
            "of\n",
            "stem\n",
            "algorithm\n",
            "that\n",
            "are\n",
            "avail\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBxldFOX_hT6"
      },
      "source": [
        "SnowBall Stemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iaRr6B_AQIz"
      },
      "source": [
        "Difference between stemmers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqWKWcwE_O5P",
        "outputId": "aad387c5-004a-44a0-cff0-357539b390d0"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer1 = SnowballStemmer(\"english\")\n",
        "stemmer2 = SnowballStemmer(\"english\", ignore_stopwords = True)\n",
        "\n",
        "print(stemmer1.stem('Having'))\n",
        "print(stemmer2.stem('Having'))\n",
        "\n",
        "print(stemmer1.stem('Generously'))\n",
        "print(SnowballStemmer('porter').stem('Generously'))\n",
        "\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "have\n",
            "having\n",
            "generous\n",
            "gener\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-Ump0NgAIrM",
        "outputId": "6cd40aee-8ffa-4362-bde8-318ae95d2bef"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "input_str = \"There are several types of stemming algorithms that are available\"\n",
        "input_str = word_tokenize(input_str)\n",
        "print(\"Stem words :\\n\")\n",
        "for word in input_str:\n",
        "  print(stemmer.stem(word)) "
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stem words :\n",
            "\n",
            "there\n",
            "are\n",
            "sever\n",
            "type\n",
            "of\n",
            "stem\n",
            "algorithm\n",
            "that\n",
            "are\n",
            "avail\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADqClohRAdo8"
      },
      "source": [
        "Lemmatizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owZ962xjAkxg"
      },
      "source": [
        "Download wordnet library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH-X1oWzAZ8K",
        "outputId": "e27782d2-1f5d-41d8-c6b6-5ca2a48a5315"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "03x3vF_pAiZq",
        "outputId": "0e44be4a-d965-493e-a144-1270045824fe"
      },
      "source": [
        "import nltk\n",
        "\n",
        "lemma = nltk.wordnet.WordNetLemmatizer()\n",
        "lemma.lemmatize('Played')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Played'"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ygur00ZhA6qH"
      },
      "source": [
        "Object standardisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYqmjzIGA3Wp",
        "outputId": "00e63b17-1157-433a-a971-0d2f57949067"
      },
      "source": [
        "lookup_dict = {'rt':'Retweet','dm':'direct message','awsm':'awesome','luv':'love','@':'at','info':'information'}\n",
        "\n",
        "def lookup_words(input_text):\n",
        "  \n",
        "  words = input_text.split()\n",
        "  new_words=[]\n",
        "  \n",
        "  for word in words:\n",
        "  \n",
        "    if word.lower() in lookup_dict:\n",
        "  \n",
        "      word = lookup_dict[word.lower()]\n",
        "  \n",
        "    new_words.append(word)\n",
        "  \n",
        "  new_text = \" \".join(new_words)\n",
        "  return new_text\n",
        "\n",
        "print(lookup_words(\"RT : we are going to CCD @ MG road!! DM me for more info on this!! luv to all , its been an awsm day!\"))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retweet : we are going to CCD at MG road!! direct message me for more information on this!! love to all , its been an awesome day!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wagbgqhCpnK"
      },
      "source": [
        "This is just the beginning of the text preprocessing phase."
      ]
    }
  ]
}